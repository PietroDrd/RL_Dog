### GENERAL ###

import torch

### SKRL-RL STUFF ###

from skrl.agents.torch.ppo import PPO, PPO_DEFAULT_CONFIG
from skrl.envs.wrappers.torch import Wrapper, wrap_env
        # equal to --> from omni.isaac.lab_tasks.utils.wrappers.skrl import SkrlVecEnvWrapper

from skrl.resources.preprocessors.torch import RunningStandardScaler
from skrl.memories.torch import RandomMemory
from skrl.agents.torch import Agent
from skrl.models.torch import Model, GaussianMixin, DeterministicMixin
from skrl.trainers.torch import Trainer, SequentialTrainer, ParallelTrainer, StepTrainer 
from skrl.utils import set_seed

### ISAACLAB CLASSES ###
from omni.isaac.lab.envs     import ManagerBasedRLEnv

############# CHECK IF TO-DO ###############
# omni.isaac.lab_tasks.utils.wrappers.skrl.process_skrl_cfg(cfg: dict) â†’ dict:
"""
from omni.isaac.lab_tasks.utils.wrappers.skrl import procss_skrl_cfg
process_skrl_cfg(ALIENGO_ENV_CFG)   # IF ANY (must adapt it)
"""

class PPO_v1:
    def __init__(self, env: ManagerBasedRLEnv, config=PPO_DEFAULT_CONFIG, device = "cpu"):
        self.env = wrap_env(env)    # SKRL: wrapper = "auto", by default,  otherwise--> "isaaclab"
        self.config = config
        self.device = device
        self.num_envs = env.num_envs   #needed for MEMORY of PPO, num_envs comes from "args" of the env object
        self.agent = self._create_agent()
        

    def _create_model_nn(self):

        #  NO, PPO REQUIRES A SHARED (Stochast+Determ) Models -->class Shared(GaussianMixin, DeterministicMixin, Model):
        #  IMPLEMENT IT FOR FUCK SAKE
        
        # return {
        #     "policy": torch.nn.Sequential(
        #         torch.nn.Linear(self.env.observation_space.shape[0], 256),
        #         torch.nn.ReLU(),
        #         torch.nn.Linear(256, 128),
        #         torch.nn.ReLU(),
        #         torch.nn.Linear(128, 64),
        #         torch.nn.ReLU(),
        #         torch.nn.Linear(64, self.env.action_space.shape[0])
        #     )
        # }
        self.no = None
        return pass
    
    def _create_preprocessor(self):
        return {
            "observation_scaler": RunningStandardScaler(shape=self.env.observation_space.shape)
        }

    def _create_agent(self):
        model_nn_ = self._create_model_nn()
        preprocessor_ = self._create_preprocessor()
        
        # instantiate a memory as rollout buffer (any memory can be used for this)
        memory_rndm_ = RandomMemory(memory_size=24, num_envs=self.num_envs, device=self.device)

        agent = PPO(
            models=model_nn_,
            preprocessor = preprocessor_,
            memory=memory_rndm_,
            cfg=self.config,
            env=self.env,
            device=self.device
        )
        return agent
    
    def train_mine_easy(self, num_episodes=1000):
        for ep in range(num_episodes):
            obs = self.env.reset()
            done = False
            tot_reward = 0
            cnt = 0
            while not done:
                action = self.agent.act(obs)  # act if a skrl.agent.torch.ppo --> PPO's method
                """ Process the environment's states to make a decision (actions) using the main policy """
                obs, reward, done, info = self.env.step(actions=action)
                tot_reward += reward 
                cnt += 1
                if cnt % 10 == 0: print(f"Episode: {ep}, Total Reward: {tot_reward}")

    def train_sequential(self, timesteps=20000, headless=False):
        cfg_trainer= {"timesteps": timesteps, "headless": headless}
        trainer = SequentialTrainer(cfg=cfg_trainer, env=self.env, agents=self.agent)
        trainer.train()

    def train_parallel(self, timesteps=20000, headless=False):
        cfg_trainer = {"timesteps": timesteps, "headless": headless}
        trainer = ParallelTrainer(cfg=cfg_trainer, env=self.env, agents=self.agent)
        trainer.train() 


# just to have a look about the values, this is ignored by the code since its after 
PPO_DEFAULT_CONFIG = {
    "rollouts": 16,                 # number of rollouts before updating
    "learning_epochs": 8,           # number of learning epochs during each update
    "mini_batches": 2,              # number of mini batches during each learning epoch

    "discount_factor": 0.99,        # discount factor (gamma)
    "lambda": 0.95,                 # TD(lambda) coefficient (lam) for computing returns and advantages

    "learning_rate": 1e-3,                  # learning rate
    "learning_rate_scheduler": None,        # learning rate scheduler class (see torch.optim.lr_scheduler)
    "learning_rate_scheduler_kwargs": {},   # learning rate scheduler's kwargs (e.g. {"step_size": 1e-3})

    "state_preprocessor": None,             # state preprocessor class (see skrl.resources.preprocessors)
    "state_preprocessor_kwargs": {},        # state preprocessor's kwargs (e.g. {"size": env.observation_space})
    "value_preprocessor": None,             # value preprocessor class (see skrl.resources.preprocessors)
    "value_preprocessor_kwargs": {},        # value preprocessor's kwargs (e.g. {"size": 1})

    "random_timesteps": 0,          # random exploration steps
    "learning_starts": 0,           # learning starts after this many steps

    "grad_norm_clip": 0.5,              # clipping coefficient for the norm of the gradients
    "ratio_clip": 0.2,                  # clipping coefficient for computing the clipped surrogate objective
    "value_clip": 0.2,                  # clipping coefficient for computing the value loss (if clip_predicted_values is True)
    "clip_predicted_values": False,     # clip predicted values during value loss computation

    "entropy_loss_scale": 0.0,      # entropy loss scaling factor
    "value_loss_scale": 1.0,        # value loss scaling factor

    "kl_threshold": 0,              # KL divergence threshold for early stopping

    "rewards_shaper": None,         # rewards shaping function: Callable(reward, timestep, timesteps) -> reward
    "time_limit_bootstrap": False,  # bootstrap at timeout termination (episode truncation)

    "experiment": {
        "directory": "",            # experiment's parent directory
        "experiment_name": "",      # experiment name
        "write_interval": "auto",   # TensorBoard writing interval (timesteps)

        "checkpoint_interval": "auto",      # interval for checkpoints (timesteps)
        "store_separately": False,          # whether to store checkpoints separately

        "wandb": False,             # whether to use Weights & Biases
        "wandb_kwargs": {}          # wandb kwargs (see https://docs.wandb.ai/ref/python/init)
    }
}

# ADD TRAIN 

# ppo 1
# https://skrl.readthedocs.io/en/latest/_downloads/17f299c7b73f8d5f2c56b336c693da94/torch_velocity_anymal_c_ppo.py

# ppo2  --> EXAMPLE_anymal_PPO.py
# https://skrl.readthedocs.io/en/latest/_downloads/7f665f3e3ea3a391c065747e4d1ef288/torch_anymal_ppo.py

# skrl exampl
# https://skrl.readthedocs.io/en/latest/intro/examples.html#nvidia-isaac-lab